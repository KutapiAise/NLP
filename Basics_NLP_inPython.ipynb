{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basics:NLP_inPython.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KutapiAise/NLP/blob/master/Basics_NLP_inPython.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKo4yg0bqz9S",
        "colab_type": "text"
      },
      "source": [
        "What is Natural Language Processing?\n",
        "- Field of study focused on making sense of language\n",
        "Using statistics and computers\n",
        "- You will learn the basics of NLP\n",
        "   - Topic identification\n",
        "   - Text classification\n",
        "- NLP applications include:\n",
        "   - Chatbots\n",
        "   - Translation\n",
        "   - Sentiment analysis\n",
        "     ... and many more!\n",
        "\n",
        "\n",
        "#### What exactly are regular expressions?\n",
        "- Strings with a special syntax\n",
        "- Allow us to match patterns in other strings\n",
        "- Applications of regular expressions:\n",
        "   - Find all web links in a document\n",
        "   - Parse email addresses, remove/replace unwanted characters\n",
        "   \n",
        "- In [1]: import re\n",
        "- In [2]: re.match('abc', 'abcdef')\n",
        "- Out[2]: <_sre.SRE_Match object; span=(0, 3), match='abc'>\n",
        "\n",
        "\n",
        "#### Which pattern?\n",
        "Which of the following Regex patterns results in the following text?\n",
        "\n",
        " - my_string = \"Let's write RegEx!\"\n",
        " - re.findall(PATTERN, my_string)\n",
        "     - ['Let', 's', 'write', 'RegEx']\n",
        "\n",
        "In the IPython Shell, try replacing PATTERN with one of the below options and observe the resulting output. The re module has been pre-imported for you and my_string is available in your namespace.\n",
        "\n",
        "Answer: PATTERN = r\"\\w+\"\n",
        "\n",
        "\n",
        "#### Common regex patterns (7)\n",
        "pattern matches example\n",
        "- \\w+ = word :'Magic'\n",
        "- \\d = digit : 9\n",
        "- \\s = space : ' '\n",
        "- .* = wildcard :'username74'\n",
        "- \\+ or * = greedy match : 'aaaaaa'\n",
        "- \\S = not space : 'no_spaces'\n",
        "- [a-z] = lowercase group 'abcdefg'\n",
        "\n",
        "#### Python's re Module\n",
        "* re module\n",
        "* split: split a string on regex\n",
        "* findall: find all patterns in a string\n",
        "* search: search for a pattern\n",
        "* match: match an entire string or substring based on a pattern\n",
        "* Pattern first, and the string second\n",
        "* May return an iterator, string, or match object\n",
        "\n",
        "\n",
        "### * Practicing regular expressions: re.split() and re.findall()\n",
        "Now you'll get a chance to write some regular expressions to match digits, strings and non-alphanumeric characters. Take a look at my_string first by printing it in the IPython Shell, to determine how you might best match the different steps.\n",
        "\n",
        "Note: It's important to prefix your regex patterns with r to ensure that your patterns are interpreted in the way you want them to. Else, you may encounter problems to do with escape sequences in strings. For example, \"\\n\" in Python is used to indicate a new line, but if you use the r prefix, it will be interpreted as the raw string \"\\n\" - that is, the character \"\\\" followed by the character \"n\" - and not as a new line.\n",
        "\n",
        "The regular expression module re has already been imported for you.\n",
        "\n",
        "Remember from the video that the syntax for the regex library is to always to pass the pattern first, and then the string second.\n",
        "\n",
        "\n",
        "\n",
        "* Split my_string on each sentence ending. To do this:\n",
        "   * Write a pattern called sentence_endings to match sentence endings (.?!).\n",
        "   * Use re.split() to split my_string on the pattern and print the result.\n",
        "* Find and print all capitalized words in my_string by writing a pattern called capitalized_words and using re.findall().\n",
        "   * Remember the [a-z] pattern shown in the video to match lowercase groups? Modify that pattern appropriately in order to match uppercase groups.\n",
        "* Write a pattern called spaces to match one or more spaces (\"\\s+\") and then use re.split() to split my_string on this pattern, keeping all punctuation intact. Print the result.\n",
        "* Find all digits in my_string by writing a pattern called digits (\"\\d+\") and using re.findall(). Print the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcSarNVaSQ2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2beQIPqb4E3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF_yRFkjSgZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_string=\"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4EiahFmogmG",
        "colab_type": "code",
        "outputId": "b5b429ef-c471-4ca5-bcca-c3f2ec03de06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "\n",
        "# Write a pattern to match sentence endings: sentence_endings\n",
        "sentence_endings = r\"[.?!]\"\n",
        "\n",
        "# Split my_string on sentence endings and print the result\n",
        "print(re.split(sentence_endings, my_string))\n",
        "\n",
        "# Find all capitalized words in my_string and print the result\n",
        "capitalized_words = r\"[A-Z]+\\w+\"\n",
        "print(re.findall(capitalized_words, my_string))\n",
        "\n",
        "# Split my_string on spaces and print the result\n",
        "spaces = r\"\\s+\"\n",
        "print(re.split(spaces, my_string))\n",
        "\n",
        "# Find all digits in my_string and print the result\n",
        "digits = r\"[0-9]+\"\n",
        "print(re.findall(digits, my_string))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
            "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
            "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
            "['4', '19']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m63VrTKUpzg",
        "colab_type": "text"
      },
      "source": [
        "#### What is tokenization?\n",
        "Turning a string or document into tokens (smaller chunks)\n",
        "- One step in preparing a text for NLP\n",
        "- Many different theories and rules\n",
        "- You can create your own rules using regular expressions\n",
        "\n",
        "Some examples:\n",
        "- Breaking out words or sentences\n",
        "- Separating punctuation\n",
        "- Separating all hashtags in a tweet\n",
        "\n",
        "\n",
        "#### nltk library\n",
        "nltk: natural language toolkit\n",
        "- In [1]: from nltk.tokenize import word_tokenize\n",
        "- In [2]: word_tokenize(\"Hi there!\")\n",
        "- Out[2]: ['Hi','there','!']\n",
        "\n",
        "#### Why tokenize?\n",
        "- Easier to map part of speech\n",
        "- Matching common words\n",
        "- Removing unwanted tokens\n",
        "- \"I don't like Sam's shoes.\"\n",
        "- \"I\",\"do\",\"n't\",\"like\",\"Sam\",\"'s\",\"shoes\",\".\"  ===> here you can see the posession with 's and negation with  n't\n",
        "\n",
        "#### Other nltk tokenizers\n",
        "- sent_tokenize: tokenize a document into sentences\n",
        "- regexp_tokenize: tokenize a string or document based on a regular\n",
        "  expression pattern\n",
        "- TweetTokenizer: special class just for tweet tokenization, allowing you\n",
        "to separate hashtags, mentions and lots of exclamation points!!!\n",
        "\n",
        "#### More regex practice\n",
        "Difference between re.search() and re.match()\n",
        "- In [1]: import re\n",
        "- In [2]: re.match('abc','abcde')\n",
        "- Out[2]: <_sre.SRE_Match object; span=(0, 3), match='abc'>\n",
        "- In [3]: re.search('abc','abcde')\n",
        "- Out[3]: <_sre.SRE_Match object; span=(0, 3), match='abc'>\n",
        "- In [4]: re.match('cd','abcde')\n",
        "- In [5]: re.search('cd','abcde')\n",
        "- Out[5]: <_sre.SRE_Match object; span=(2, 4), match='cd'>\n",
        "\n",
        "#### Word tokenization with NLTK\n",
        "Here, you'll be using the first scene of Monty Python's Holy Grail, which has been pre-loaded as scene_one. Feel free to check it out in the IPython Shell!\n",
        "\n",
        "Your job in this exercise is to utilize word_tokenize and sent_tokenize from nltk.tokenize to tokenize both words and sentences from Python strings - in this case, the first scene of Monty Python's Holy Grail.\n",
        "\n",
        "- Import the sent_tokenize and word_tokenize functions from nltk.tokenize.\n",
        "- Tokenize all the sentences in scene_one using the sent_tokenize() function.\n",
        "- Tokenize the fourth sentence in sentences, which you can access as sentences[3], using the word_tokenize() function.\n",
        "- Find the unique tokens in the entire scene by using word_tokenize() on scene_one and then converting it into a set using set().\n",
        "- Print the unique tokens found. This has been done for you, so hit 'Submit Answer' to see the results!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuoNYdVZt8HB",
        "colab_type": "code",
        "outputId": "8f4362b7-c8c4-4c55-a932-be5356b006ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        " !pip3 install nltk"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qnasIDAxM7y",
        "colab_type": "code",
        "outputId": "1f1f8f1f-92de-427c-98e9-30b5f7bcb95a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Import necessary modules\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdGTEnhISUeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scene_one= \"SCENE 1: [wind] [clop clop clop] \\n KING ARTHUR: Whoa there!  [clop clop clop] \\n SOLDIER #1: Halt!  Who goes there?\\n ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England! \\n SOLDIER #1: Pull the other one! \\n ARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\\n SOLDIER #1: What?  Ridden on a horse? \\n ARTHUR: Yes! \\n SOLDIER #1: You're using coconuts!\\n ARTHUR: What? SOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\\n ARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\\n SOLDIER #1: Where'd you get the coconuts?\\nARTHUR: We found them.\\n SOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\\n ARTHUR: What do you mean?\\n SOLDIER #1: Well, this is a temperate zone.\\n ARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\\n SOLDIER #1: Are you suggesting coconuts migrate?\\nARTHUR: Not at all.  They could be carried.\\nSOLDIER #1: What?  A swallow carrying a coconut?\\n ARTHUR: It could grip it by the husk!\\n SOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\\n ARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\\n SOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\\n ARTHUR: Please!\\nSOLDIER #1: Am I right?\\n ARTHUR: I'm not interested!\\n SOLDIER #2: It could be carried by an African swallow!\\n SOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\\n SOLDIER #2: Oh, yeah, I agree with that.\\n ARTHUR: Will you ask your master if he wants to join my court at Camelot?!\\n SOLDIER #1: But then of course a-- African swallows are non-migratory.\\nSOLDIER #2: Oh, yeah...\\n SOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \\n SOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\\n SOLDIER #1: No, they'd have to have it on a line.\\n SOLDIER #2: Well, simple!  They'd just use a strand of creeper! \\n SOLDIER #1: What, held under the dorsal guiding feathers?\\nSOLDIER #2: Well, why not?\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdnipragvTwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#scene_one=\"SCENE 1: [wind] [clop clop clop]  KING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\\nSOLDIER #1: Pull the other one!\\nARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\\nSOLDIER #1: What?  Ridden on a horse?\\nARTHUR: Yes!\\nSOLDIER #1: You're using coconuts!\\nARTHUR: What?\\nSOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\\nARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\\nSOLDIER #1: Where'd you get the coconuts?\\nARTHUR: We found them.\\nSOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\\nARTHUR: What do you mean?\\nSOLDIER #1: Well, this is a temperate zone.\\nARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\\nSOLDIER #1: Are you suggesting coconuts migrate?\\nARTHUR: Not at all.  They could be carried.\\nSOLDIER #1: What?  A swallow carrying a coconut?\\nARTHUR: It could grip it by the husk!\\nSOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\\nARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\\nSOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\\nARTHUR: Please!\\nSOLDIER #1: Am I right?\\nARTHUR: I'm not interested!\\nSOLDIER #2: It could be carried by an African swallow!\\nSOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\\nSOLDIER #2: Oh, yeah, I agree with that.\\nARTHUR: Will you ask your master if he wants to join my court at Camelot?!\\nSOLDIER #1: But then of course a-- African swallows are non-migratory.\\nSOLDIER #2: Oh, yeah...\\nSOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \\nSOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\\nSOLDIER #1: No, they'd have to have it on a line.\\nSOLDIER #2: Well, simple!  They'd just use a strand of creeper!\\nSOLDIER #1: What, held under the dorsal guiding feathers?\\nSOLDIER #2: Well, why not?\\n\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMndcOr-fuby",
        "colab_type": "code",
        "outputId": "305c48f0-e4f6-46c8-9ac4-b8d582eed2ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Import necessary modules\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize ,word_tokenize\n",
        "\n",
        "\n",
        "# Split scene_one into sentences: sentences\n",
        "sentences = sent_tokenize(scene_one)\n",
        "\n",
        "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
        "tokenized_sent = word_tokenize(sentences[3])\n",
        "\n",
        "# Make a set of unique tokens in the entire scene: unique_tokens\n",
        "unique_tokens = set( word_tokenize(scene_one))\n",
        "\n",
        "# Print the unique tokens result\n",
        "print(unique_tokens)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{']', 'Patsy', 'our', 'it', 'who', 'why', 'Pull', 'martin', 'Are', 'got', 'be', 'maintain', 'King', 'must', 'So', 'bring', 'Camelot', 'length', 'five', 'ridden', 'master', 'these', 'KING', 'empty', '1', 'climes', \"'d\", 'warmer', 'simple', 'But', 'Ridden', '!', 'have', 'minute', 'swallow', 'husk', 'Will', 'SCENE', '[', 'strand', 'point', 'sovereign', 'on', 'trusty', 'Uther', 'tropical', 'back', 'a', 'your', 'Please', 'Who', 'held', 'covered', 'No', 'am', \"n't\", 'fly', 'grip', 'We', 'castle', 'one', 'with', 'by', 'coconuts', 'does', 'court', 'Mercea', 'all', 'Where', 'do', ',', 'ounce', 'but', 'creeper', 'is', 'winter', 'get', 'if', 'that', 'order', 'anyway', 'could', 'using', 'yeah', 'ratios', 'its', '.', 'velocity', 'of', 'swallows', 'beat', 'migrate', 'other', '...', '?', 'grips', 'and', 'this', 'yet', 'seek', 'bangin', 'you', 'tell', 'will', 'dorsal', 'carrying', 'kingdom', \"'m\", 'question', 'every', 'the', 'Whoa', 'he', '--', 'SOLDIER', 'You', 'needs', 'clop', 'maybe', 'land', 'Supposing', 'ask', \"'em\", 'goes', 'England', 'horse', '#', 'Pendragon', 'non-migratory', 'breadth', 'temperate', 'them', 'there', 'In', 'together', 'times', 'Found', 'right', 'I', 'Halt', 'go', 'feathers', 'defeator', 'they', 'sun', 'weight', 'servant', 'lord', 'Yes', 'Court', 'They', 'at', 'African', 'That', 'agree', 'me', 'to', 'son', 'south', 'A', 'where', 'here', 'interested', 'mean', '2', 'It', 'What', 'just', 'Oh', 'join', 'Not', 'two', 'carried', 'are', 'ARTHUR', 'may', 'wind', 'in', \"'ve\", 'from', 'European', 'air-speed', 'since', \"'\", 'found', 'pound', 'coconut', 'matter', 'wings', 'Wait', 'strangers', 'bird', 'Well', 'course', 'Britons', 'house', 'under', 'snows', 'guiding', 'use', 'line', 'knights', 'second', 'Am', 'then', 'suggesting', 'carry', ':', 'through', 'my', 'search', \"'re\", 'zone', 'Arthur', 'not', 'Listen', 'halves', 'The', 'or', 'speak', 'forty-three', \"'s\", 'an', 'wants', 'plover', 'Saxons'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYeDEjFOfxWc",
        "colab_type": "text"
      },
      "source": [
        "#### More regex with re.search()\n",
        "In this exercise, you'll utilize re.search() and re.match() to find specific tokens. Both search and match expect regex patterns, similar to those you defined in an earlier exercise. You'll apply these regex library methods to the same Monty Python text from the nltk corpora.\n",
        "\n",
        "You have both scene_one and sentences available from the last exercise; now you can use them with re.search() and re.match() to extract and match more text.\n",
        "\n",
        "\n",
        "- Use re.search() to search for the first occurrence of the word \"coconuts\" in scene_one. Store the result in match.\n",
        "- Print the start and end indexes of match using its .start() and .end() methods, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4r3rjTjf8Z3",
        "colab_type": "code",
        "outputId": "8459aed9-0540-49fb-d55e-bcac04f71b63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
        "match = re.search(\"coconuts\", scene_one)\n",
        "\n",
        "# Print the start and end indexes of match\n",
        "print(match.start(),match.end())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "592 600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgmT11O6p8Qa",
        "colab_type": "text"
      },
      "source": [
        "- Write a regular expression called pattern1 to find anything in square brackets.\n",
        "- Use re.search() with the pattern to find the first text in scene_one in square brackets in the scene. Print the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l5qqTAhp7u2",
        "colab_type": "code",
        "outputId": "3a8326a1-0144-48d2-df94-3878ae976a8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Write a regular expression to search for anything in square brackets: pattern1\n",
        "pattern1 = r\"\\[.*]\"\n",
        "\n",
        "# Use re.search to find the first text in square brackets\n",
        "print(re.search(pattern1, scene_one))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<_sre.SRE_Match object; span=(9, 32), match='[wind] [clop clop clop]'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_slAYVraqVwA",
        "colab_type": "text"
      },
      "source": [
        "- Create a pattern to match the script notation (e.g. Character:), assigning the result to pattern2. Remember that you will want to match any words or spaces that precede the : (such as the space within SOLDIER #1:).\n",
        "- Use re.match() with your new pattern to find and print the script notation in the fourth line. The tokenized sentences are available in your namespace as sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQx9E9Hzrgdv",
        "colab_type": "code",
        "outputId": "244d836d-528a-4e99-8b7f-831792d053ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Find the script notation at the beginning of the fourth sentence and print it\n",
        "pattern2 = r\"[\\w\\s]+:\"\n",
        "print(re.search(pattern2 ,sentences[3]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<_sre.SRE_Match object; span=(0, 7), match='ARTHUR:'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbd1fKnxMW8W",
        "colab_type": "text"
      },
      "source": [
        "###  Advanced tokenization with NLTK and regex\n",
        "\n",
        "\n",
        "* #### Choosing a tokenizer\n",
        "\n",
        "Given the following string, which of the below patterns is the best tokenizer? If possible, you want to retain sentence punctuation as separate tokens, but have '#1' remain a single token.\n",
        "\n",
        "my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n",
        "\n",
        "\n",
        "The string is available in your workspace as my_string, and the patterns have been pre-loaded as pattern1, pattern2, pattern3, and pattern4, respectively.\n",
        "\n",
        "Additionally, regexp_tokenize has been imported from nltk.tokenize. You can use regexp_tokenize(string, pattern) with my_string and one of the patterns as arguments to experiment for yourself and see which is the best tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aufg8EaURZ0L",
        "colab_type": "code",
        "outputId": "c6c23565-b63c-4322-924d-2a188d3c099f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "from nltk.tokenize import regexp_tokenize\n",
        "\n",
        "\n",
        "my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n",
        "\n",
        "regexp_tokenize(my_string,r\"(\\w+|#\\d|\\?|!)\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['SOLDIER',\n",
              " '#1',\n",
              " 'Found',\n",
              " 'them',\n",
              " '?',\n",
              " 'In',\n",
              " 'Mercea',\n",
              " '?',\n",
              " 'The',\n",
              " 'coconut',\n",
              " 's',\n",
              " 'tropical',\n",
              " '!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5yiY6jtR2lG",
        "colab_type": "text"
      },
      "source": [
        "* ### Regex with NLTK tokenization\n",
        "\n",
        "\n",
        "Twitter is a frequently used source for NLP text and tasks. In this exercise, you'll build a more complex tokenizer for tweets with hashtags and mentions using nltk and regex. The nltk.tokenize.TweetTokenizer class gives you some extra methods and attributes for parsing tweets.\n",
        "\n",
        "Here, you're given some example tweets to parse using both TweetTokenizer and regexp_tokenize from the nltk.tokenize module. These example tweets have been pre-loaded into the variable tweets. Feel free to explore it in the IPython Shell!\n",
        "\n",
        "Unlike the syntax for the regex library, with nltk_tokenize() you pass the pattern as the second argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfzubPZdTrFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets=['This is the best #nlp exercise ive found online! #python',\n",
        " '#NLP is super fun! <3 #learning',\n",
        " 'Thanks @datacamp :) #nlp #python']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgOGLHAXRi1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Import the necessary modules\n",
        "from nltk.tokenize import TweetTokenizer,regexp_tokenize\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define a regex pattern to find hashtags: pattern1\n",
        "pattern1 = r\"#\\w+\"\n",
        "# Use the pattern on the first tweet in the tweets list\n",
        "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
        "print(hashtags)\n",
        "\n",
        "\n",
        "# Write a pattern that matches both mentions (@) and hashtags\n",
        "pattern2 = r\"([@#]\\w+)\"\n",
        "# Use the pattern on the last tweet in the tweets list\n",
        "mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
        "print(mentions_hashtags)\n",
        "\n",
        "# Use the TweetTokenizer to tokenize all tweets into one list\n",
        "tknzr = TweetTokenizer()\n",
        "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
        "print(all_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4zHM9ySXRjP",
        "colab_type": "text"
      },
      "source": [
        "* #### Non-ascii tokenization\n",
        "\n",
        "\n",
        "In this exercise, you'll practice advanced tokenization by tokenizing some non-ascii based text. You'll be using German with emoji!\n",
        "\n",
        "Here, you have access to a string called german_text, which has been printed for you in the Shell. Notice the emoji and the German characters!\n",
        "\n",
        "The following modules have been pre-imported from nltk.tokenize: regexp_tokenize and word_tokenize.\n",
        "\n",
        "Unicode ranges for emoji are:\n",
        "\n",
        "('\\U0001F300'-'\\U0001F5FF'), ('\\U0001F600-\\U0001F64F'), ('\\U0001F680-\\U0001F6FF'), and ('\\u2600'-\\u26FF-\\u2700-\\u27BF')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX1HOTpeUEl-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "german_text =\"Wann gehen wir Pizza essen? 🍕 Und fährst du mit Über? 🚕\"\n",
        "\n",
        "from nltk.tokenize import regexp_tokenize,word_tokenize\n",
        "\n",
        "# Tokenize and print all words in german_text\n",
        "all_words = word_tokenize(german_text)\n",
        "print(all_words)\n",
        "\n",
        "# Tokenize and print only capital words\n",
        "capital_words = r\"[A-ZÜ]\\w+\"\n",
        "print(regexp_tokenize(german_text, capital_words))\n",
        "\n",
        "# Tokenize and print only emoji\n",
        "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
        "print(regexp_tokenize(german_text,emoji))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6aEyWYRZ664",
        "colab_type": "text"
      },
      "source": [
        "### * Charting practice\n",
        "Try using your new skills to find and chart the number of words per line in the script using matplotlib. The Holy Grail script is loaded for you, and you need to use regex to find the words per line.\n",
        "\n",
        "Using list comprehensions here will speed up your computations. For example: my_lines = [tokenize(l) for l in lines] will call a function tokenize on each line in the list lines. The new transformed list will be saved in the my_lines variable.\n",
        "\n",
        "You have access to the entire script in the variable holy_grail. Go for it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTXv7TueaVOy",
        "colab_type": "code",
        "outputId": "79e8256e-50fd-4a71-b6d0-59dc1810c2b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "holy_grail=\"Charting practice Try using your new skills to find and chart the number of words per line in the script using matplotlib. The Holy Grail script is loaded for you, and you need to use regex to find the words per line.Using list comprehensions here will speed up your computations. For example: my_lines = [tokenize(l) for l in lines] will call a function tokenize on each line in the list lines. The new transformed list will be saved in the my_lines variable.You have access to the entire script in the variable holy_grail. Go for it!\"\n",
        "\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "# Split the script into lines: lines\n",
        "lines = holy_grail.split('\\n')\n",
        "\n",
        "# Replace all script lines for speaker\n",
        "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
        "lines = [re.sub(pattern, '', l) for l in lines]\n",
        "\n",
        "# Tokenize each line: tokenized_lines\n",
        "tokenized_lines = [regexp_tokenize(s,r\"\\w+\") for s in lines]\n",
        "\n",
        "# Make a frequency list of lengths: line_num_words\n",
        "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
        "\n",
        "# Plot a histogram of the line lengths\n",
        "plt.hist(line_num_words)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ByzXPjecxOT",
        "colab_type": "text"
      },
      "source": [
        "## Word counts with bag-of-words\n",
        "\n",
        "\n",
        "Bag-of-words picker\n",
        "It's time for a quick check on your understanding of bag-of-words. Which of the below options, with basic nltk tokenization, map the bag-of-words for the following text?\n",
        "\n",
        "\"The cat is in the box. The cat box.\"\n",
        "\n",
        "('The', 2), ('box', 2), ('.', 2), ('cat', 2), ('is', 1), ('in', 1), ('the', 1)\n",
        "\n",
        "Remember: no preprocessing to remove capitals or punctuation!\n",
        "\n",
        "\n",
        "### * Building a Counter with bag-of-words\n",
        "In this exercise, you'll build your first (in this course) bag-of-words counter using a Wikipedia article, which has been pre-loaded as article. Try doing the bag-of-words without looking at the full article text, and guessing what the topic is! If you'd like to peek at the title at the end, we've included it as article_title. Note that this article text has had very little preprocessing from the raw Wikipedia database entry.\n",
        "\n",
        "word_tokenize has been imported for you.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6BxIfHgcI-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Counter\n",
        "from collections import Counter\n",
        "\n",
        "# Tokenize the article: tokens\n",
        "tokens = word_tokenize(article)\n",
        "\n",
        "# Convert the tokens into lowercase: lower_tokens\n",
        "lower_tokens = [tkn.lower() for tkn in tokens]\n",
        "\n",
        "# Create a Counter with the lowercase tokens: bow_simple\n",
        "bow_simple = Counter(lower_tokens)\n",
        "\n",
        "# Print the 10 most common tokens\n",
        "print(bow_simple.most_common(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgZQU4nM2Icd",
        "colab_type": "text"
      },
      "source": [
        "### Text preprocessing practice\n",
        "\n",
        "Now, it's your turn to apply the techniques you've learned to help clean up text for better NLP results. You'll need to remove stop words and non-alphabetic characters, lemmatize, and perform a new bag-of-words on your cleaned text.\n",
        "\n",
        "You start with the same tokens you created in the last exercise: lower_tokens. You also have the Counter class imported."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT5M21j83X6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import WordNetLemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Retain alphabetic words: alpha_only\n",
        "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
        "\n",
        "# Remove all stop words: no_stops\n",
        "no_stops = [t for t in alpha_only if t not in english_stops]\n",
        "\n",
        "# Instantiate the WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize all tokens into a new list: lemmatized\n",
        "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
        "\n",
        "# Create the bag-of-words: bow\n",
        "bow = Counter(lemmatized)\n",
        "\n",
        "# Print the 10 most common tokens\n",
        "print(bow.most_common(10))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNuhdz8O363J",
        "colab_type": "text"
      },
      "source": [
        "#### Introduction to gensim\n",
        "\n",
        "What is gensim?\n",
        "Popular open-source NLP library\n",
        "Uses top academic models to perform complex tasks\n",
        "Building document or word vectors\n",
        "Performing topic identification and document comparison \n",
        "\n",
        "What are word vectors?\n",
        "What are word vectors and how do they help with NLP?\n",
        "\n",
        " Word vectors are multi-dimensional mathematical representations of words created using deep learning methods. They give us insight into relationships between words in a corpus.\n",
        "\n",
        "\n",
        "#### * Creating and querying a corpus with gensim\n",
        "It's time to apply the methods you learned in the previous video to create your first gensim dictionary and corpus!\n",
        "\n",
        "You'll use these data structures to investigate word trends and potential interesting topics in your document set. To get started, we have imported a few additional messy articles from Wikipedia, which were preprocessed by lowercasing all words, tokenizing them, and removing stop words and punctuation. These were then stored in a list of document tokens called articles. You'll need to do some light preprocessing and then generate the gensim dictionary and corpus.\n",
        "\n",
        "\n",
        "- Obtain the id for \"computer\" from dictionary. To do this, use its .token2id method which returns ids from text, and then chain .get() which returns tokens from ids. Pass in \"computer\" as an argument to .get().\n",
        "- Use a list comprehension in which you iterate over articles to create a gensim MmCorpus from dictionary.\n",
        "- In the output expression, use the .doc2bow() method on dictionary with article as the argument.\n",
        "- Print the first 10 word ids with their frequency counts from the fifth document. This has been done for you, so hit 'Submit Answer' to see the results!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXRVpXBk35hT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Dictionary\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "\n",
        "# Create a Dictionary from the articles: dictionary\n",
        "dictionary = Dictionary(articles)\n",
        "\n",
        "# Select the id for \"computer\": computer_id\n",
        "computer_id = dictionary.token2id.get(\"computer\")\n",
        "\n",
        "# Use computer_id with the dictionary to print the word\n",
        "print(dictionary.get(computer_id))\n",
        "\n",
        "# Create a MmCorpus: corpus\n",
        "corpus = [dictionary.doc2bow(article) for article in articles]\n",
        "\n",
        "# Print the first 10 word ids with their frequency counts from the fifth document\n",
        "print(corpus[4][:10])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4FYelT3VOaL",
        "colab_type": "text"
      },
      "source": [
        "### Gensim bag-of-words\n",
        "Now, you'll use your new gensim corpus and dictionary to see the most common terms per document and across all documents. You can use your dictionary to look up the terms. Take a guess at what the topics are and feel free to explore more documents in the IPython Shell!\n",
        "\n",
        "You have access to the dictionary and corpus objects you created in the previous exercise, as well as the Python defaultdict and itertools to help with the creation of intermediate data structures for analysis.\n",
        "\n",
        "defaultdict allows us to initialize a dictionary that will assign a default value to non-existent keys. By supplying the argument int, we are able to ensure that any non-existent keys are automatically assigned a default value of 0. This makes it ideal for storing the counts of words in this exercise.\n",
        "\n",
        "itertools.chain.from_iterable() allows us to iterate through a set of sequences as if they were one continuous sequence. Using this function, we can easily iterate through our corpus object (which is a list of lists).\n",
        "\n",
        "The fifth document from corpus is stored in the variable doc, which has been sorted in descending order.\n",
        "\n",
        "Instructions:\n",
        "\n",
        "Using the first for loop, print the top five words of bow_doc using each word_id with the dictionary alongside word_count.\n",
        "\n",
        "The word_id can be accessed using the .get() method of dictionary.\n",
        "Create a defaultdict called total_word_count in which the keys are all the token ids (word_id) and the values are the sum of their occurrence across all documents (word_count).\n",
        "\n",
        "Remember to specify int when creating the defaultdict, and inside the second for loop, increment each word_id of total_word_count by word_count.\n",
        "\n",
        "Create a sorted list from the defaultdict, using words across the entire corpus. To achieve this, use the .items() method on total_word_count inside sorted().\n",
        "\n",
        "Similar to how you printed the top five words of bow_doc earlier, print the top five words of sorted_word_count as well as the number of occurrences of each word across all the documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbsrxIkyaGfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the fifth document: doc\n",
        "doc = corpus[4]\n",
        "\n",
        "# Sort the doc for frequency: bow_doc\n",
        "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
        "\n",
        "# Print the top 5 words of the document alongside the count\n",
        "for word_id, word_count in bow_doc[:5]:\n",
        "    print(dictionary.get(word_id), word_count)\n",
        "    \n",
        "# Create the defaultdict: total_word_count\n",
        "total_word_count = defaultdict(int)\n",
        "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
        "    total_word_count[word_id] += word_count\n",
        "    \n",
        "# Create a sorted list from the defaultdict: sorted_word_count\n",
        "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
        "\n",
        "# Print the top 5 words across all documents alongside the count\n",
        "for word_id, word_count in sorted_word_count[:5]:\n",
        "    print(dictionary.get(word_id), word_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6mhGmCYfAG3",
        "colab_type": "text"
      },
      "source": [
        "#### What is tf-idf?\n",
        "You want to calculate the tf-idf weight for the word \"computer\", which appears five times in a document containing 100 words. Given a corpus containing 200 documents, with 20 documents mentioning the word \"computer\", tf-idf can be calculated by multiplying term frequency with inverse document frequency.\n",
        "\n",
        "Term frequency = percentage share of the word compared to all tokens in the document Inverse document frequency = logarithm of the total number of documents in a corpora divided by the number of documents containing the term\n",
        "\n",
        "Which of the below options is correct?\n",
        "\n",
        "Answer:  (5 / 100) * log(200 / 20)\n",
        "\n",
        "\n",
        "#### * Tf-idf with Wikipedia\n",
        "Now it's your turn to determine new significant terms for your corpus by applying gensim's tf-idf. You will again have access to the same corpus and dictionary objects you created in the previous exercises - dictionary, corpus, and doc. Will tf-idf make for more interesting results on the document level?\n",
        "\n",
        "TfidfModel has been imported for you from gensim.models.tfidfmodel.\n",
        "\n",
        "\n",
        "- Initialize a new TfidfModel called tfidf using corpus.\n",
        "- Use doc to calculate the weights. You can do this by passing [doc] to tfidf.\n",
        "Print the first five term ids with weights.\n",
        "\n",
        "- Sort the term ids and weights in a new list from highest to lowest weight. This has been done for you.\n",
        "- Using your pre-existing dictionary, print the top five weighted words (term_id) from sorted_tfidf_weights, along with their weighted score (weight)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDEoWr3zgnZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a new TfidfModel using the corpus: tfidf\n",
        "tfidf = TfidfModel(corpus)\n",
        "\n",
        "# Calculate the tfidf weights of doc: tfidf_weights\n",
        "tfidf_weights = tfidf[doc]\n",
        "\n",
        "# Print the first five weights\n",
        "print(tfidf_weights[:5])\n",
        "\n",
        "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
        "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
        "\n",
        "# Print the top 5 weighted words\n",
        "for term_id, weight in sorted_tfidf_weights[:5]:\n",
        "    print(dictionary.get(term_id), weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnFWi5R9g7QS",
        "colab_type": "text"
      },
      "source": [
        "### Named Entity Recognition\n",
        "\n",
        "NER with NLTK\n",
        "You're now going to have some fun with named-entity recognition! A scraped news article has been pre-loaded into your workspace. Your task is to use nltk to find the named entities in this article.\n",
        "\n",
        "What might the article be about, given the names you found?\n",
        "\n",
        "Along with nltk, sent_tokenize and word_tokenize from nltk.tokenize have been pre-imported.\n",
        "\n",
        "\n",
        "instructions:\n",
        "\n",
        "Tokenize article into sentences.\n",
        "Tokenize each sentence in sentences into words using a list comprehension.\n",
        "Inside a list comprehension, tag each tokenized sentence into parts of speech using nltk.pos_tag().\n",
        "Chunk each tagged sentence into named-entity chunks using nltk.ne_chunk_sents(). Along with pos_sentences, specify the additional keyword argument binary=True.\n",
        "Loop over each sentence and each chunk, and test whether it is a named-entity chunk by testing if it has the attribute label, and if the chunk.label() is equal to \"NE\". If so, print that chunk.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgguIC2UrixP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize the article into sentences: sentences\n",
        "sentences = sent_tokenize(article)\n",
        "\n",
        "# Tokenize each sentence into words: token_sentences\n",
        "token_sentences = [word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
        "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
        "\n",
        "# Create the named entity chunks: chunked_sentences\n",
        "chunked_sentences = nltk.ne_chunk_sents(pos_sentences,binary=True)\n",
        "\n",
        "# Test for stems of the tree with 'NE' tags\n",
        "for sent in chunked_sentences:\n",
        "    for chunk in sent:\n",
        "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
        "            print(chunk)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yMhj0XdtbiH",
        "colab_type": "text"
      },
      "source": [
        "#### * Charting practice\n",
        "In this exercise, you'll use some extracted named entities and their groupings from a series of newspaper articles to chart the diversity of named entity types in the articles.\n",
        "\n",
        "You'll use a defaultdict called ner_categories, with keys representing every named entity group type, and values to count the number of each different named entity type. You have a chunked sentence list called chunked_sentences similar to the last exercise, but this time with non-binary category names.\n",
        "\n",
        "You can use hasattr() to determine if each chunk has a 'label' and then simply use the chunk's .label() method as the dictionary key.\n",
        "\n",
        "instructions:\n",
        "\n",
        "Create a defaultdict called ner_categories, with the default type set to int.\n",
        "\n",
        "Fill up the dictionary with values for each of the keys. Remember, the keys will represent the label().\n",
        "\n",
        "In the outer for loop, iterate over chunked_sentences, using sent as your iterator variable.\n",
        "In the inner for loop, iterate over sent. If the condition is true, increment the value of each key by 1.\n",
        "Remember to use the chunk's .label() method as the key!\n",
        "For the pie chart labels, create a list called labels from the keys of ner_categories, which can be accessed using .keys().\n",
        "\n",
        "Use a list comprehension to create a list called values, using the .get() method on ner_categories to compute the values of each label v.\n",
        "\n",
        "Use plt.pie() to create a pie chart for each of the NER categories. Along with values and labels=labels, pass the extra keyword arguments autopct='%1.1f%%' and startangle=140 to add percentages to the chart and rotate the initial start angle.\n",
        "\n",
        "This step has been done for you.\n",
        "Display your pie chart. Was the distribution what you expected?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D71pQxsyvrDn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the defaultdict: ner_categories\n",
        "ner_categories = defaultdict(int)\n",
        "\n",
        "# Create the nested for loop\n",
        "for sent in chunked_sentences:\n",
        "    for chunk in sent:\n",
        "        if hasattr(chunk, 'label'):\n",
        "            ner_categories[chunk.label()] += 1\n",
        "            \n",
        "# Create a list from the dictionary keys for the chart labels: labels\n",
        "labels = list(ner_categories.keys())\n",
        "\n",
        "# Create a list of the values: values\n",
        "values = [ner_categories.get(v) for v in labels]\n",
        "\n",
        "# Create the pie chart\n",
        "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
        "\n",
        "# Display the chart\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6zO4vJOv7P_",
        "colab_type": "text"
      },
      "source": [
        "* Stanford library with NLTK\n",
        "\n",
        "-When using the Stanford library with NLTK, what is needed to get started?\n",
        "\n",
        "NLTK, the Stanford Java Libraries and some environment variables to help with integration.\n",
        "press\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3G7ER7wh06xs",
        "colab_type": "text"
      },
      "source": [
        "### Comparing NLTK with spaCy NER\n",
        "Using the same text you used in the first exercise of this chapter, you'll now see the results using spaCy's NER annotator. How will they compare?\n",
        "\n",
        "The article has been pre-loaded as article. To minimize execution times, you'll be asked to specify the keyword arguments tagger=False, parser=False, matcher=False when loading the spaCy model, because you only care about the entity in this exercise.\n",
        "\n",
        "instructions:\n",
        "\n",
        "Import spacy.\n",
        "Load the 'en' model using spacy.load(). Specify the additional keyword arguments tagger=False, parser=False, matcher=False.\n",
        "Create a spacy document object by passing article into nlp().\n",
        "Using ent as your iterator variable, iterate over the entities of doc and print out the labels (ent.label_) and text (ent.text)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWS11Gvq1H3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import spacy\n",
        "import spacy\n",
        "\n",
        "# Instantiate the English model: nlp\n",
        "nlp = spacy.load('en',agger=False, parser=False, matcher=False)\n",
        "\n",
        "# Create a new document: doc\n",
        "doc = nlp(article)\n",
        "\n",
        "# Print all of the found entities and their labels\n",
        "for ent in doc.ents:\n",
        "    print(ent.label_,ent.text)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs51qqYx1Y6D",
        "colab_type": "text"
      },
      "source": [
        "### spaCy NER Categories\n",
        "Which are the extra categories that spacy uses compared to nltk in its named-entity recognition?\n",
        "\n",
        "NORP, CARDINAL, MONEY, WORKOFART, LANGUAGE, EVENT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouWH1ZhQ4dAJ",
        "colab_type": "text"
      },
      "source": [
        "### Multilingual NER with polyglot\n",
        "\n",
        "What is polyglot?\n",
        "- NLP library which uses word vectors\n",
        "- Why polyglot?\n",
        "     - Vectors for many different languages\n",
        "      More than 130!\n",
        "      \n",
        "      \n",
        " ### French NER with polyglot I\n",
        "In this exercise and the next, you'll use the polyglot library to identify French entities. The library functions slightly differently than spacy, so you'll use a few of the new things you learned in the last video to display the named entity text and category.\n",
        "\n",
        "You have access to the full article string in article. Additionally, the Text class of polyglot has been imported from polyglot.text.\n",
        "\n",
        "instructions:\n",
        "\n",
        "-Using the article string in article, create a new Text object called txt.\n",
        "- Iterate over txt.entities and print each entity, ent.\n",
        "- Print the type() of ent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcbsCAyN6Yi2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a new text object using Polyglot's Text class: txt\n",
        "txt = Text(article)\n",
        "\n",
        "# Print each of the entities found\n",
        "for ent in txt.entities:\n",
        "    print(ent)\n",
        "    \n",
        "# Print the type of ent\n",
        "print(type(ent))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9qBCmGF6dzg",
        "colab_type": "text"
      },
      "source": [
        "### French NER with polyglot II\n",
        "Here, you'll complete the work you began in the previous exercise.\n",
        "\n",
        "Your task is to use a list comprehension to create a list of tuples, in which the first element is the entity tag, and the second element is the full string of the entity text.\n",
        "\n",
        "instructions:\n",
        "\n",
        "Use a list comprehension to create a list of tuples called entities.\n",
        "The output expression of your list comprehension should be a tuple.\n",
        "The first element of each tuple is the entity tag, which you can access using its .tag attribute.\n",
        "The second element is the full string of the entity text, which you can access using .join(ent).\n",
        "Your iterator variable should be ent, and you should iterate over all of the entities of the polyglot Text object, txt.\n",
        "Print entities to see what you've created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxOsz38Y7KfT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the list of tuples: entities\n",
        "entities = [(ent.tag, ' '.join(ent)) for ent in txt.entities]\n",
        "\n",
        "# Print entities\n",
        "print(entities)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAgxrXfX7PaQ",
        "colab_type": "text"
      },
      "source": [
        "### Spanish NER with polyglot\n",
        "You'll continue your exploration of polyglot now with some Spanish annotation. This article is not written by a newspaper, so it is your first example of a more blog-like text. How do you think that might compare when finding entities?\n",
        "\n",
        "The Text object has been created as txt, and each entity has been printed, as you can see in the IPython Shell.\n",
        "\n",
        "Your specific task is to determine how many of the entities contain the words \"Márquez\" or \"Gabo\" - these refer to the same person in different ways!\n",
        "\n",
        "\n",
        "instructions:\n",
        "\n",
        "Iterate over all of the entities of txt, using ent as your iterator variable.\n",
        "Check whether the entity contains \"Márquez\" or \"Gabo\". If it does, increment count. Don't forget to include the accented á in \"Márquez\"!\n",
        "Hit 'Submit Answer' to see what percentage of entities refer to Gabriel García Márquez (aka Gabo)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIGlFpkb8LVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize the count variable: count\n",
        "count = 0\n",
        "\n",
        "# Iterate over all the entities\n",
        "for ent in txt.entities:\n",
        "    # Check whether the entity contains 'Márquez' or 'Gabo'\n",
        "    if 'Márquez' in ent or 'Gabo' in ent:\n",
        "        # Increment count\n",
        "        count+=1\n",
        "\n",
        "# Print count\n",
        "print(count)\n",
        "\n",
        "# Calculate the percentage of entities that refer to \"Gabo\": percentage\n",
        "percentage = count / len(txt.entities)\n",
        "print(percentage)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOcedztg8K2X",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}